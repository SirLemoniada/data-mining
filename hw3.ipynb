{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pandas is a data manipulation library\n",
    "import numpy as np #provides numerical arrays and functions to manipulate the arrays efficiently\n",
    "import matplotlib.pyplot as plt # data visualization library\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: k-means Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use for the evaluation of the correct answer seed fixing for random generators in sklearn and numpy. Hence, make sure that you have the newest versions (sklearn version 1.2.0 and numpy version 1.24.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 show scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 show numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your versions don't match, the following commands (or their anaconda version) could help to get the newest stable release. If you need help with this, please ask the TAs during instruction hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scikit-learn --upgrade\n",
    "!pip3 install numpy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions generating the datasets are given here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMoons(epsilon, n):\n",
    "    moons, labels = datasets.make_moons(n_samples=n, noise=epsilon, random_state=7)\n",
    "    return moons, labels, 2\n",
    "def generateBlobs(epsilon, n):\n",
    "    blobs, labels = datasets.make_blobs(n_samples=n,centers=3, cluster_std=[epsilon + 1, epsilon + 1.5, epsilon + 0.5], random_state=54)\n",
    "    return blobs, labels, 3\n",
    "def generateAniso(epsilon,n):\n",
    "    # Anisotropicly distributed data\n",
    "    A, labels = datasets.make_blobs(n_samples=n, random_state=170)\n",
    "    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "    aniso = np.dot(A, transformation)\n",
    "    return aniso, labels, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the centroid initialization here. Right now, it returns a random sample from the datapoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_centroids_greedy_pp(D,r,l=10):\n",
    "    '''\n",
    "        :param r: (int) number of centroids (clusters)\n",
    "        :param D: (np-array) the data matrix\n",
    "        :param l: (int) number of centroid candidates in each step\n",
    "        :return: (np-array) 'X' the selected centroids from the dataset\n",
    "    '''   \n",
    "    #Do not change the seed\n",
    "    rng =  np.random.default_rng(seed=7) # use this random generator to sample the candidates (sampling according to given probabilities can be done via rng.choice(..))\n",
    "    n,d = D.shape\n",
    "\n",
    "    indexes = rng.integers(low=0, high=n, size=r)\n",
    "    X = np.array(D[indexes,:]).T\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the $k$-means implementation from the lecture accompanying notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSS(D,X,Y):\n",
    "    return np.sum((D- Y@X.T)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getY(labels):\n",
    "    '''\n",
    "        Compute the cluster assignment matrix Y from the categorically encoded labels\n",
    "    '''\n",
    "    Y = np.eye(max(labels)+1)[labels]\n",
    "    return Y\n",
    "def update_centroid(D,Y):\n",
    "    cluster_sizes = np.diag(Y.T@Y).copy()\n",
    "    cluster_sizes[cluster_sizes==0]=1\n",
    "    return D.T@Y/cluster_sizes\n",
    "def update_assignment(D,X):\n",
    "    dist = np.sum((np.expand_dims(D,2) - X)**2,1)\n",
    "    labels = np.argmin(dist,1)\n",
    "    return getY(labels)\n",
    "def kmeans(D,r, X_init, epsilon=0.00001, t_max=10000):\n",
    "    X = X_init.copy()\n",
    "    Y = update_assignment(D,X)\n",
    "    rss_old = RSS(D,X,Y) +2*epsilon\n",
    "    t=0\n",
    "    #Looping as long as difference of objective function values is larger than epsilon\n",
    "    while rss_old - RSS(D,X,Y) > epsilon and t < t_max-1:\n",
    "        rss_old = RSS(D,X,Y)\n",
    "        X = update_centroid(D,Y)\n",
    "        Y = update_assignment(D,X)\n",
    "        t+=1\n",
    "    print(t,\"iterations\")\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=500\n",
    "D_blobs, labels_blobs, r_blobs = generateBlobs(0.05,n)\n",
    "D_moons, labels_moons, r_moons = generateMoons(0.05,n)\n",
    "D_aniso, labels_aniso, r_aniso = generateAniso(0.05,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, labels, r = D_blobs, labels_blobs, r_blobs\n",
    "X_init = init_centroids_greedy_pp(D,r)\n",
    "X,Y = kmeans(D,r, X_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the clustering. The initial centroids are marked in red, and the final centroids are marked in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.scatter(D[:, 0], D[:, 1], c=np.argmax(Y,axis=1), s=10)\n",
    "plt.scatter(X_init.T[:, 0], X_init.T[:, 1], c='red', s=50, marker = 'D')\n",
    "plt.scatter(X.T[:, 0], X.T[:, 1], c='blue', s=50, marker = 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "D_m, labels_m, r_m = D_moons, labels_moons, r_moons\n",
    "X_init_m = init_centroids_greedy_pp(D_m,r_m)\n",
    "X_m,Y_m = kmeans(D_m,r_m, X_init_m)\n",
    "\n",
    "D_a, labels_a, r_a = D_aniso, labels_aniso, r_aniso\n",
    "X_init_a = init_centroids_greedy_pp(D_a,r_a)\n",
    "X_a,Y_a = kmeans(D_a,r_a, X_init_a)\n",
    "\n",
    "predicted_labels = Y.argmax(axis=1)\n",
    "nmi_score = normalized_mutual_info_score(labels, predicted_labels)\n",
    "nmi_score_rounded = round(nmi_score, 2)\n",
    "\n",
    "print(\"NMI score(Blob):\", nmi_score_rounded)\n",
    "\n",
    "predicted_labels_m = Y_m.argmax(axis=1)\n",
    "nmi_score_m = normalized_mutual_info_score(labels_m, predicted_labels_m)\n",
    "nmi_score_rounded_m = round(nmi_score_m, 2)\n",
    "\n",
    "predicted_labels_a = Y_a.argmax(axis=1)\n",
    "nmi_score_a = normalized_mutual_info_score(labels_a, predicted_labels_a)\n",
    "nmi_score_rounded_a = round(nmi_score_a, 2)\n",
    "\n",
    "print(\"NMI score(Aniso):\", nmi_score_rounded_a)\n",
    "print(\"NMI score(Moon):\", nmi_score_rounded_m)\n",
    "\n",
    "# Q1a)A-Correct, B-Correct, C-Correct, D-Wrong\n",
    "# Q1b)0.95, 0.19, 0.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# Q2\n",
    "#A-Correct\n",
    "data = {'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'X': [13.40, 6.29, 10.68,12.04, 6.10, 0.77,7.64, 2.26, -0.31,0.29],\n",
    "        'Y': ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No']}\n",
    "yes_data = [data['X'][i] for i in range(len(data['X'])) if data['Y'][i] == 'Yes']\n",
    "class_specific_mean = sum(yes_data) / len(yes_data)\n",
    "print(\"The class-specific mean of X of the companies that are 'Yes':\", class_specific_mean)\n",
    "\n",
    "\n",
    "\n",
    "#B-Incorrect\n",
    "\n",
    "# Assume we have data on past companies, where the first column is the percent profit from the previous year and the second column is whether or not the company issued a dividend\n",
    "# 0 = No, 1 = Yes\n",
    "data = np.array([[13.40,1], [6.29,1], [10.68,1], [12.04,1], [6.10,1], [0.77,1], [7.64,0], [2.26,0], [-0.31,0], [0.29,0]])\n",
    "\n",
    "# Split the data into the feature (X) and target (y) variables\n",
    "X = data[:,0]\n",
    "y = data[:,1]\n",
    "\n",
    "# Create an instance of the Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the model on the data\n",
    "gnb.fit(X.reshape(-1,1), y)\n",
    "\n",
    "# Make a prediction on a new data point (unit)\n",
    "X_new = np.array([4])\n",
    "pred_proba = gnb.predict_proba(X_new.reshape(-1,1))\n",
    "\n",
    "# calculate the mean and standard deviation of X\n",
    "mean = np.mean(X)\n",
    "sigma = math.sqrt(np.sum((x - mean) ** 2 for x in X) / (len(X) - 1))\n",
    "\n",
    "# calculate the posterior odds ratio\n",
    "posterior_odds_ratio = pred_proba[0][0] / pred_proba[0][1]\n",
    "print(\"The posterior odds ratio for a company who hypothetically had last year's profit X = 4 is {:.4f}\".format(posterior_odds_ratio))\n",
    "\n",
    "\n",
    "\n",
    "#C-Correct\n",
    "\n",
    "data = np.array([[13.40,1], [6.29,1], [10.68,1], [12.04,1], [6.10,1], [0.77,1], [7.64,0], [2.26,0], [-0.31,0], [0.29,0]])\n",
    "\n",
    "# Split the data into the feature (X) and target (y) variables\n",
    "X = data[:,0].reshape(-1,1)\n",
    "y = data[:,1]\n",
    "\n",
    "# Create an instance of the Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the model on the data\n",
    "gnb.fit(X, y)\n",
    "\n",
    "# Make a prediction on a new data point (unit)\n",
    "X_new = np.array([0.77]).reshape(-1,1)\n",
    "pred = gnb.predict(X_new)\n",
    "\n",
    "if pred == 0:\n",
    "    print(\"According to the Naive Bayes model, the sixth unit (ID=6) is classified in the negative class (Class 'No').\")\n",
    "else:\n",
    "    print(\"According to the Naive Bayes model, the sixth unit (ID=6) is classified in the positive class (Class 'Yes').\")\n",
    "\n",
    "    \n",
    "#D-Incorrect  \n",
    "    \n",
    "data = np.array([[13.40,1], [6.29,1], [10.68,1], [12.04,1], [6.10,1], [0.77,1], [7.64,0], [2.26,0], [-0.31,0], [0.29,0]])\n",
    "\n",
    "# Split the data into the feature (X) and target (y) variables\n",
    "X = data[:,0].reshape(-1,1)\n",
    "y = data[:,1]\n",
    "\n",
    "# Create an instance of the Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the model on the data\n",
    "gnb.fit(X, y)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred = gnb.predict(X)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "print(\"The accuracy of the Naive Bayes model on the training data is {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data - 80:20 ratio\n",
    "\n",
    "Use also for this exercise the latest stable version of sklearn because of the use of the random generator for the train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X , y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "fitted_tree = tree.fit(X_train, y_train)\n",
    "y_pred = fitted_tree.predict(X_test)\n",
    "\n",
    "print(\"Training split input- \", X_train.shape)\n",
    "print(\"Testing split input- \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to visualize the decision tree (the argument is a fitted sklearn DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iris_tree(decision_tree):\n",
    "        fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)\n",
    "        plot_tree(decision_tree=decision_tree, feature_names = [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"], \n",
    "                     class_names =[\"setosa\", \"versicolor\", \"verginica\"] , filled = True , precision = 2, rounded = True)\n",
    "plot_iris_tree(fitted_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A - True, B - True, C - False, D - False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise : Stochastic Gradient Descent\n",
    "\n",
    "Scikit learn provides two classes `SGDRegressor` and `SGDClassifier` which use stochastic gradient descent to carry out linear regression and classification respectively.\n",
    "\n",
    "These models are especially useful in these situations:\n",
    "- with very large datasets\n",
    "- with streaming data (they support online learning)\n",
    "- with datasets with sparse features\n",
    "\n",
    "SGD is sensitive to learning rate and the scale of the features. It's advisable to z-score the features, and to tune the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the dataset of 50,000 songs. The prediction task is to guess the year the song was made based on 90 timbre features extracted from the audio. The year is in the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = np.load(\"C:/Users/snipe/OneDrive - TU Eindhoven/UNI Y2/Q2/JBI030/HW3/songs50k.npy\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(songs[:,1:], songs[:,0], test_size=1/3, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate the SGD classifier on the songs dataset. Take the following steps:\n",
    "- z-score the training and validation features\n",
    "- find good settings for learning rate type and learning rate initial value.\n",
    "  - r-squared on validation data \n",
    "  - mean absolute error on validation data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the data using the StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "X_train_sc=scaler.fit_transform(X_train)\n",
    "X_val_sc=scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use also for this exercise the latest stable version of sklearn because of the use of the random generator for the train and test split.\n",
    "\n",
    "Use the SGDRegressor with a random state of 123, that is evaluate the parameter settings like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=[\"optimal\", \"constant\",\"invscaling\"]\n",
    "eta0= [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "loss = [\"huber\", \"squared_loss\"]\n",
    "model = SGDRegressor(learning_rate=learning_rate[0],eta0=eta0[0],random_state=123,loss=loss[0])\n",
    "model.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  combinationsR2(learning_rate, eta0, loss):\n",
    "    dictR2 = {}\n",
    "    for i in learning_rate:\n",
    "        for k in eta0:\n",
    "            for l in loss:\n",
    "                model = SGDRegressor(learning_rate=i,eta0=k,random_state=123,loss=l)\n",
    "                model.fit(X_train_sc, y_train)\n",
    "                y_pred = model.predict(X_val_sc)\n",
    "                dictR2[i,k,l] = r2_score(y_val, y_pred)\n",
    "    return dictR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  combinationsMSE(learning_rate, eta0, loss):\n",
    "    dictMSE = {}\n",
    "    for i in learning_rate:\n",
    "        for k in eta0:\n",
    "            for l in loss:\n",
    "                model = SGDRegressor(learning_rate=i,eta0=k,random_state=123,loss=l)\n",
    "                model.fit(X_train_sc, y_train)\n",
    "                y_pred = model.predict(X_val_sc)\n",
    "                dictMSE[i,k,l] = mean_absolute_error(y_val, y_pred)\n",
    "    return dictMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictR2 = combinationsR2(learning_rate, eta0, loss)\n",
    "dictMAE = combinationsMSE(learning_rate, eta0, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A False\n",
    "bestR2 = max(dictR2.values())\n",
    "bestR2 == 0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B False\n",
    "lowestR2 = dictR2['optimal', 1, 'huber']\n",
    "lowestR2 == min(dictR2.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C False\n",
    "lowestMAE = dictMAE['optimal', 1, 'huber']\n",
    "lowestMAE == min(dictMAE.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D True\n",
    "dictR2['invscaling', 0.01, 'huber'] - dictR2['invscaling', 0.1, 'huber']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default SGDRegressor tries to minimize the standard linear regression error function, that is sum of squared error. However this can be changed, via the `loss=` parameter. When `loss='squared_loss'`, sum of squared errors will be used. Other error functions available include [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) (`loss='huber'`). Compared to squared loss, huber focuses less on outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise : Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `scatter_pictures` displays a scatter plot where the coordinates are given by `Y` and the data points are visualized by pictures of the faces defined in `D_approx`. You can try using `D_approx=D` as argument or you could insert the approximation of the pictures as computed by the low-rank matrix factorization (SVD) into `D_approx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_pictures(D_approx, Y):\n",
    "    zoom = 0.4\n",
    "    fig, ax = plt.subplots(figsize=(16,16))\n",
    "    for i in range(D.shape[0]):\n",
    "        image = D_approx[i,:].reshape((64, 64))\n",
    "        im = OffsetImage(image, cmap=plt.cm.gray,zoom=zoom)\n",
    "        ab = AnnotationBbox(im, (Y[i,0], Y[i,1]), xycoords='data', frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "        ax.update_datalim([(Y[i,0], Y[i,1])])\n",
    "        ax.autoscale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces()\n",
    "D = faces.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_pictures(D, np.random.rand(D.shape[0],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=2\n",
    "U, s, V = np.linalg.svd(D,full_matrices=True)\n",
    "Y=U[:,0:r]*np.sqrt(s[0:r])\n",
    "X=V.T[:,0:r]*np.sqrt(s[0:r])\n",
    "D_trunc = Y@X.T\n",
    "scatter_pictures(D_trunc,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=100\n",
    "U, s, V = np.linalg.svd(D,full_matrices=True)\n",
    "Y=U[:,0:r]*np.sqrt(s[0:r])\n",
    "X=V.T[:,0:r]*np.sqrt(s[0:r])\n",
    "D_trunc = Y@X.T\n",
    "scatter_pictures(D_trunc,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualization with r=2 we can see that one of the features corresponds to whether light is coming from left or right side of the picture (Y-axis). The other axis can only be distincted when r is much higher - like r=100. Then we can instantly notice that pictures on the right represent people with darker skin tone, less light, or dark facial hair, while on the left we can only see caucasians without any hair in the frame and high exposition of the picture.\n",
    "\n",
    "- X-axis - Overall amount of light/dark pixels (e.g. skin tone, black hair, picture exposition)\n",
    "- Y-axis - light on left/right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent space (Y) represents feature intensity in specific pictures. First column represents how bright the picture is - the higher the value, the brighter it is. Second column informs how skewed to the left the origin of light is - the higher the value, the more light we see coming from the left side andless from the right side."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
